@online{Minecraft,
    author = {OpenAI},
    title = {Learning to play Minecraft with Video PreTraining},
    url = {https://openai.com/research/vpt},
    year = {2022}
}

@software{Gymnasium,
    author = {Towers, Mark and Terry, Jordan K and Kwiatkowski, Ariel and Balis, John U. and de Cola, Gianluca and Deleu, Tristan and Goulão, Manuel and Kallinteris, Andreas and KG, Arjun and Krimmel, Markus and Perez-Vicente, Rodrigo and Pierré, Andrea and Schulhoff, Sander and Tai, Jun Jet and Tan, Andrew Jin Shen and Younis, Omar G.},
    license = {MIT},
    title = {Gymnasium},
    url = {https://github.com/Farama-Foundation/Gymnasium}
}

@software{Cart-pole,
    author = {Towers, Mark and Terry, Jordan K and Kwiatkowski, Ariel and Balis, John U. and de Cola, Gianluca and Deleu, Tristan and Goulão, Manuel and Kallinteris, Andreas and KG, Arjun and Krimmel, Markus and Perez-Vicente, Rodrigo and Pierré, Andrea and Schulhoff, Sander and Tai, Jun Jet and Tan, Andrew Jin Shen and Younis, Omar G.},
    license = {MIT},
    title = {Cart Pole},
    url = {https://gymnasium.farama.org/environments/classic_control/cart_pole/}
}

@software{Basic-usage,
    author = {Towers, Mark and Terry, Jordan K and Kwiatkowski, Ariel and Balis, John U. and de Cola, Gianluca and Deleu, Tristan and Goulão, Manuel and Kallinteris, Andreas and KG, Arjun and Krimmel, Markus and Perez-Vicente, Rodrigo and Pierré, Andrea and Schulhoff, Sander and Tai, Jun Jet and Tan, Andrew Jin Shen and Younis, Omar G.},
    license = {MIT},
    title = {Basic Usage},
    url = {https://gymnasium.farama.org/content/basic_usage/}
}

@online{Human-level,
    author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
    title = {Human-level control through deep reinforcement learning},
    url = {https://doi.org/10.1038/nature14236},
    year = {2015}
}

@article{Neuronlike,
    author = {Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},
    title = {Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems},
    journal = {IEEE},
    year = {1983}
}

@online{REINFORCE,
    author = {Yoon, Chris},
    title = {Deriving Policy Gradients and Implementing REINFORCE},
    url = {https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63},
    year = {2018}
}

@online{HF-approaches,
    author = {Huggingface},
    title = {Two main approaches for solving RL problems},
    url = {https://huggingface.co/learn/deep-rl-course/unit1/two-methods}
}

@online{HF-policy,
    author = {Huggingface},
    title = {What are the policy-based methods?},
    url = {https://huggingface.co/learn/deep-rl-course/unit4/what-are-policy-based-methods}
}

@online{HF-value,
    author = {Huggingface},
    title = {Two types of value-based methods},
    url = {https://huggingface.co/learn/deep-rl-course/unit2/two-types-value-based-methods}
}

@online{HF-bellman,
    author = {Huggingface},
    title = {The Bellman Equation: simplify our value estimation},
    url = {https://huggingface.co/learn/deep-rl-course/unit2/bellman-equation}
}

@online{Q-intro,
    author = {Amine, Amrani},
    title = {A gentle introduction to Reinforcement Learning},
    year = {2020},
    url = {https://aamrani1999.medium.com/a-gentle-introduction-to-reinforcement-learning-d26cba6455f7}
}

@online{Q-traditional,
    author = {Amine, Amrani},
    title = {Q-Learning Algorithm: From Explanation to Implementation},
    year = {2020},
    url = {https://towardsdatascience.com/q-learning-algorithm-from-explanation-to-implementation-cdbeda2ea187}
}

@online{Q-deep,
    author = {Huggingface},
    title = {The Deep Q-Learning Algorithm},
    url = {https://huggingface.co/learn/deep-rl-course/unit3/deep-q-algorithm}
}


@misc{Double-Q,
      title={Deep Reinforcement Learning with Double Q-learning},
      author={Hado van Hasselt and Arthur Guez and David Silver},
      year={2015},
      eprint={1509.06461},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{Technical-Q-learning,
	abstract = {{\$}{\$}{$\backslash$}mathcal{\{}Q{\}}{\$}{\$}-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	date = {1992/05/01},
	date-added = {2024-02-15 12:43:01 +0100},
	date-modified = {2024-02-15 12:43:01 +0100},
	doi = {10.1023/A:1022676722315},
	id = {Watkins1992},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {3},
	pages = {279--292},
	title = {Technical Note: Q-Learning},
	url = {https://doi.org/10.1023/A:1022676722315},
	volume = {8},
	year = {1992},
	bdsk-url-1 = {https://doi.org/10.1023/A:1022676722315}}
