{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Value-based agent using Apple MLX"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53c89e80d06ac9e1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-11T09:44:24.879242Z",
     "start_time": "2024-02-11T09:44:24.231975Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import imageio\n",
    "import mlx.core as mx\n",
    "import gymnasium as gym\n",
    "import mlx.optimizers as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from agents.mlx_value import MinibatchDeepQ"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cart-pole environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "970db53c864ab740"
  },
  {
   "cell_type": "markdown",
   "source": [
    "|        | TYPE             | VALUES | DESCRIPTION                                                                     |\n",
    "|--------|------------------|--------|---------------------------------------------------------------------------------|\n",
    "| Action | ndarray<br/>(1,) | {0, 1} | Direction to push cart.<br/>0: left<br/>1: right                                |\n",
    "| Observation | ndarray<br/>(4,) | float  | 1. Cart position<br/>2. Cart velocity<br/>3. Pole angle<br/>4. Angular velocity |\n",
    "| Reward |  | float | Reward is given for every step taken, including the termination step.<br/>Each step provides a reward of +1 |\n",
    "| Termination &<br/>Truncation |  | boolean | Pole Angle exceeds ± 0.2095 rad<br/>Cart Position exceeds ± 2.4 (i.e., edge of the display)<br/>Episode length is greater than 500 |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "811d6d17402feb84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "environment = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T09:44:24.885055Z",
     "start_time": "2024-02-11T09:44:24.879910Z"
    }
   },
   "id": "82fb48c186edca6",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "NOTE:\n",
    "\n",
    "The mini-batch value represents how many _full_ games to train on from the agents memor. \n",
    "Therefore, an additional parameter `TRAIN_EVERY` is introduced to control how often the agent \n",
    "should learn from the memorized games. Although, even with this parameter, the training time \n",
    "increases significantly as the agent improves and has to learn from longer and longer games."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "502492d920829821"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "GAMES = 5000\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "EXPLORATION_RATE = 1.0\n",
    "EXPLORATION_DECAY = 0.995\n",
    "EXPLORATION_MIN = 0.01\n",
    "\n",
    "MINIBATCH = 64\n",
    "TRAIN_EVERY = 10\n",
    "\n",
    "MEMORY = 1500\n",
    "RESET_Q_EVERY = 250\n",
    "\n",
    "NETWORK = {\"inputs\": 4, \"outputs\": 2, \"nodes\": [15, 30]}\n",
    "OPTIMIZER = {\"optim\": optim.RMSprop, \"lr\": 0.0025}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T09:44:24.889381Z",
     "start_time": "2024-02-11T09:44:24.885519Z"
    }
   },
   "id": "2476ac8633268c19",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "value_agent = MinibatchDeepQ(\n",
    "    network=NETWORK, optimizer=OPTIMIZER,\n",
    "    gamma=GAMMA, batch_size=MINIBATCH, memory=MEMORY,\n",
    "    exploration_rate=EXPLORATION_RATE, exploration_decay=EXPLORATION_DECAY, exploration_min=EXPLORATION_MIN\n",
    ")\n",
    "\n",
    "_value_agent = MinibatchDeepQ(\n",
    "    network=NETWORK, optimizer=OPTIMIZER,\n",
    "    gamma=GAMMA, batch_size=MINIBATCH, memory=MEMORY,\n",
    "    exploration_rate=EXPLORATION_RATE, exploration_decay=EXPLORATION_DECAY, exploration_min=EXPLORATION_MIN\n",
    ")\n",
    "_value_agent.update(value_agent.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T09:44:24.889836Z",
     "start_time": "2024-02-11T09:44:24.888400Z"
    }
   },
   "id": "4efdbf0dbc4e327f",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "checkpoint = GAMES // 10\n",
    "metrics = {\n",
    "    \"steps\": mx.zeros(GAMES),\n",
    "    \"losses\": mx.zeros(GAMES // TRAIN_EVERY),\n",
    "    \"exploration\": mx.zeros(GAMES)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T09:44:24.901673Z",
     "start_time": "2024-02-11T09:44:24.891227Z"
    }
   },
   "id": "3e9591009b6a7d65",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to cast Python instance of type <class 'agents.mlx_value.MinibatchDeepQ'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 24\u001B[0m\n\u001B[1;32m     19\u001B[0m value_agent\u001B[38;5;241m.\u001B[39mmemorize(steps)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m game \u001B[38;5;241m%\u001B[39m TRAIN_EVERY \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;66;03m# loss, grad = loss_func(_value_agent)\u001B[39;00m\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;66;03m# value_agent.optimizer.update(value_agent, grad)\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mvalue_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_value_agent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m     metrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlosses\u001B[39m\u001B[38;5;124m\"\u001B[39m][game \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m TRAIN_EVERY \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m game \u001B[38;5;241m%\u001B[39m RESET_Q_EVERY \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Desktop/UNIVERSITET/1 Spesialpensum/Kode/reinforcement-learning/agents/mlx_value.py:211\u001B[0m, in \u001B[0;36mMinibatchDeepQ.learn\u001B[0;34m(self, network)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;66;03m# BACKPROPAGATION\u001B[39;00m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------------------------\u001B[39;00m\n\u001B[1;32m    209\u001B[0m loss \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mlosses\u001B[38;5;241m.\u001B[39mmse_loss(actual, optimal)\n\u001B[0;32m--> 211\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;66;03m# EXPLORATION RATE DECAY\u001B[39;00m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------------------------\u001B[39;00m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplore[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrate\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplore[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdecay\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplore[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrate\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    217\u001B[0m                            \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplore[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.12/site-packages/mlx/optimizers.py:28\u001B[0m, in \u001B[0;36mOptimizer.update\u001B[0;34m(self, model, gradients)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate\u001B[39m(\u001B[38;5;28mself\u001B[39m, model: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmlx.nn.Module\u001B[39m\u001B[38;5;124m\"\u001B[39m, gradients: \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m     20\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Apply the gradients to the parameters of the model and update the\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;124;03m    model with the new parameters.\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;124;03m                          via :func:`mlx.nn.value_and_grad`.\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m     model\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgradients\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.12/site-packages/mlx/optimizers.py:79\u001B[0m, in \u001B[0;36mOptimizer.apply_gradients\u001B[0;34m(self, gradients, parameters)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_initialized:\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit(gradients)\n\u001B[0;32m---> 79\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtree_map\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_single\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradients\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.12/site-packages/mlx/utils.py:54\u001B[0m, in \u001B[0;36mtree_map\u001B[0;34m(fn, tree, is_leaf, *rest)\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m     50\u001B[0m         k: tree_map(fn, child, \u001B[38;5;241m*\u001B[39m(r[k] \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m rest), is_leaf\u001B[38;5;241m=\u001B[39mis_leaf)\n\u001B[1;32m     51\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, child \u001B[38;5;129;01min\u001B[39;00m tree\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m     52\u001B[0m     }\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtree\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrest\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.12/site-packages/mlx/optimizers.py:224\u001B[0m, in \u001B[0;36mRMSprop.apply_single\u001B[0;34m(self, gradient, parameter, state)\u001B[0m\n\u001B[1;32m    221\u001B[0m v \u001B[38;5;241m=\u001B[39m alpha \u001B[38;5;241m*\u001B[39m v \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m alpha) \u001B[38;5;241m*\u001B[39m mx\u001B[38;5;241m.\u001B[39msquare(gradient)\n\u001B[1;32m    222\u001B[0m state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mv\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m v\n\u001B[0;32m--> 224\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparameter\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mmx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Unable to cast Python instance of type <class 'agents.mlx_value.MinibatchDeepQ'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for game in range(1, GAMES + 1):\n",
    "    \n",
    "    state = mx.array(environment.reset()[0])  # noqa\n",
    "    terminated = truncated = False\n",
    "    \n",
    "    # LEARNING FROM GAME\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    steps = 0\n",
    "    while not (terminated or truncated):\n",
    "        steps += 1\n",
    "        action = value_agent.action(state)\n",
    "        new_state, reward, terminated, truncated, _ = environment.step(action.item())\n",
    "        new_state = mx.array(new_state)\n",
    "\n",
    "        value_agent.remember(state, action, new_state, mx.array([reward]))\n",
    "        state = new_state\n",
    "    value_agent.memorize(steps)\n",
    "    \n",
    "    if game % TRAIN_EVERY == 0:\n",
    "        # loss, grad = loss_func(_value_agent)\n",
    "        # value_agent.optimizer.update(value_agent, grad)\n",
    "        loss = value_agent.learn(network=_value_agent)\n",
    "        metrics[\"losses\"][game // TRAIN_EVERY - 1] = loss.item()\n",
    "    \n",
    "    if game % RESET_Q_EVERY == 0:\n",
    "        _value_agent.update(value_agent.parameters())\n",
    "\n",
    "    # METRICS\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    metrics[\"steps\"][game-1] = steps\n",
    "    metrics[\"exploration\"][game-1] = value_agent.explore[\"rate\"]\n",
    "    \n",
    "    if game % checkpoint == 0 or game == GAMES:\n",
    "        _mean_steps = metrics[\"steps\"][max(0, game-checkpoint-1):game-1].mean().item()\n",
    "        _mean_loss = (metrics[\"losses\"][max(0, (game-checkpoint-1) // TRAIN_EVERY):game // \n",
    "                                                                                  TRAIN_EVERY]\n",
    "                      .mean().item())\n",
    "\n",
    "        print(f\"Game {game:>6} {int(game/GAMES * 100):>16} % \\n\"\n",
    "              f\"{'-'*30} \\n\"\n",
    "              f\" > Average steps: {int(_mean_steps):>12} \\n\"\n",
    "              f\" > Average loss: {_mean_loss:>13.4f} \\n \")\n",
    "        \n",
    "print(f\"Total training time: {time.time()-start:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T09:44:25.066671Z",
     "start_time": "2024-02-11T09:44:24.894673Z"
    }
   },
   "id": "b4abfe2d67240d1f",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visualisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e6dd43949968ed9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def moving_average(data, window_size=50):\n",
    "    \"\"\"Compute moving average with given window size of the data.\"\"\"\n",
    "    half_window = window_size // 2\n",
    "    return [(data[max(0, i-half_window):min(len(data), i+half_window)]).mean().item() \n",
    "            for i in range(len(data))]\n",
    "\n",
    "steps = moving_average(metrics[\"steps\"])\n",
    "losses = moving_average(metrics[\"losses\"])\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "fig.suptitle(\"Deep Q-Learning agent\")\n",
    "\n",
    "ax[0].axhline(y=500, color=\"red\", linestyle=\"dotted\", linewidth=1)\n",
    "ax[0].plot(steps, color=\"black\", linewidth=1)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_title(\"Average steps per game\")\n",
    "\n",
    "ax[1].plot(mx.linspace(0, GAMES, len(losses)), losses, color=\"black\", linewidth=1)\n",
    "ax[1].set_xlabel(\"Game nr.\")\n",
    "ax[1].set_title(\"Average loss\")\n",
    "\n",
    "ax_2 = ax[1].twinx()\n",
    "ax_2.plot(metrics[\"exploration\"], color=\"gray\", linewidth=0.5)\n",
    "ax_2.set_ylabel(\"Exploration rate\")\n",
    "ax_2.yaxis.label.set_color('gray')\n",
    "ax_2.tick_params(axis='y', colors='gray')\n",
    "\n",
    "for i in range(0, GAMES, GAMES // 10):\n",
    "    ax[0].axvline(x=i, color='gray', linewidth=0.5)\n",
    "    ax[1].axvline(x=i, color='gray', linewidth=0.5)\n",
    "\n",
    "plt.savefig(\"./static/images/mlx-dqn.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T09:44:25.055846Z"
    }
   },
   "id": "510380cc5f3c5685",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### In action"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96a3a9859771c84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "state = mx.array(environment.reset()[0])\n",
    "\n",
    "images = []\n",
    "terminated = truncated = False\n",
    "while not (terminated or truncated):\n",
    "    action = mx.argmax(value_agent(state)).item()\n",
    "    \n",
    "    state, reward, terminated, truncated, _ = environment.step(action)\n",
    "    state = mx.array(state)\n",
    "\n",
    "    images.append(environment.render())\n",
    "_ = imageio.mimsave('./static/images/mlx-dqn.gif', images, duration=25)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T09:44:25.056994Z"
    }
   },
   "id": "4183f64c47eb5090",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./static/images/mlx-dqn.gif\" width=\"1000\" height=\"1000\" />"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9f592c07276b7f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "environment.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-11T09:44:25.057738Z"
    }
   },
   "id": "2f63ff4b67fe2c7",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
