{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Policy-based Agent using PyTorch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53c89e80d06ac9e1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-08T11:56:11.282884Z",
     "start_time": "2024-02-08T11:56:10.273492Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from agents import PolicyGradientAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cart-pole environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "970db53c864ab740"
  },
  {
   "cell_type": "markdown",
   "source": [
    "|        | TYPE             | VALUES | DESCRIPTION                                                                     |\n",
    "|--------|------------------|--------|---------------------------------------------------------------------------------|\n",
    "| Action | ndarray<br/>(1,) | {0, 1} | Direction to push cart.<br/>0: left<br/>1: right                                |\n",
    "| Observation | ndarray<br/>(4,) | float  | 1. Cart position<br/>2. Cart velocity<br/>3. Pole angle<br/>4. Angular velocity |\n",
    "| Reward |  | float | Reward is given for every step taken, including the termination step.<br/>Each step provides a reward of +1 |\n",
    "| Termination &<br/>Truncation |  | boolean | Pole Angle exceeds ± 0.2095 rad<br/>Cart Position exceeds ± 2.4 (i.e., edge of the display)<br/>Episode length is greater than 500 |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "811d6d17402feb84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "environment = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T11:56:11.287468Z",
     "start_time": "2024-02-08T11:56:11.282287Z"
    }
   },
   "id": "82fb48c186edca6",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "502492d920829821"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "GAMES = 10000\n",
    "\n",
    "INPUTS = 4\n",
    "OUTPUTS = 2\n",
    "NODES = [15, 30]\n",
    "\n",
    "OPTIMIZER = {\"optim\": torch.optim.RMSprop, \"lr\": 0.00025}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T11:56:11.290257Z",
     "start_time": "2024-02-08T11:56:11.288288Z"
    }
   },
   "id": "2476ac8633268c19",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "policy_agent = PolicyGradientAgent(\n",
    "        network={\"inputs\": INPUTS, \"outputs\": OUTPUTS, \"nodes\": NODES},\n",
    "        optimizer=OPTIMIZER\n",
    ")\n",
    "\n",
    "checkpoint = GAMES // 10\n",
    "metrics = {\"steps\": torch.zeros(GAMES), \"gradients\": torch.zeros(GAMES)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T11:56:11.698447Z",
     "start_time": "2024-02-08T11:56:11.292761Z"
    }
   },
   "id": "4efdbf0dbc4e327f",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game   1000               10 % \n",
      "------------------------------ \n",
      " > Average steps:           29 \n",
      " > Average gradients:  -0.2848 \n",
      " \n",
      "Game   2000               20 % \n",
      "------------------------------ \n",
      " > Average steps:          102 \n",
      " > Average gradients:  -1.5363 \n",
      " \n",
      "Game   3000               30 % \n",
      "------------------------------ \n",
      " > Average steps:          284 \n",
      " > Average gradients:  -3.2185 \n",
      " \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m     observation \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(observation, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m     16\u001B[0m     policy_agent\u001B[38;5;241m.\u001B[39mmemorize(logarithm, reward)\n\u001B[0;32m---> 17\u001B[0m gradient \u001B[38;5;241m=\u001B[39m policy_agent\u001B[38;5;241m.\u001B[39mlearn()\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# METRICS\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# --------------------------------------------------\u001B[39;00m\n\u001B[1;32m     22\u001B[0m metrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msteps\u001B[39m\u001B[38;5;124m\"\u001B[39m][game\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m steps\n",
      "File \u001B[0;32m~/Desktop/UNIVERSITET/1 Spesialpensum/Kode/reinforcement-learning/agents.py:248\u001B[0m, in \u001B[0;36mPolicyGradientAgent.learn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;66;03m# BACKPROPAGATION\u001B[39;00m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;66;03m# --------------------------------------------------\u001B[39;00m\n\u001B[1;32m    244\u001B[0m \u001B[38;5;66;03m# The gradient is then used to update the Agent's policy. This is done by backpropagating\u001B[39;00m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;66;03m# with the optimizer using the gradient.\u001B[39;00m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m--> 248\u001B[0m gradient\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory \u001B[38;5;241m=\u001B[39m {key: [] \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory\u001B[38;5;241m.\u001B[39mkeys()}\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum-py11/lib/python3.11/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    523\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    524\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum-py11/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    267\u001B[0m     tensors,\n\u001B[1;32m    268\u001B[0m     grad_tensors_,\n\u001B[1;32m    269\u001B[0m     retain_graph,\n\u001B[1;32m    270\u001B[0m     create_graph,\n\u001B[1;32m    271\u001B[0m     inputs,\n\u001B[1;32m    272\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    273\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    274\u001B[0m )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for game in range(1, GAMES + 1):\n",
    "    \n",
    "    state = torch.tensor(environment.reset()[0], dtype=torch.float32)  # noqa\n",
    "    terminated = truncated = False\n",
    "    \n",
    "    # LEARNING FROM GAME\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    steps = 0\n",
    "    while not (terminated or truncated):\n",
    "        steps += 1\n",
    "        action, logarithm = policy_agent.action(state)\n",
    "        state, reward, terminated, truncated, _ = environment.step(action)\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        policy_agent.memorize(logarithm, reward)\n",
    "    gradient = policy_agent.learn()\n",
    "    \n",
    "    # METRICS\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    metrics[\"steps\"][game-1] = steps\n",
    "    metrics[\"gradients\"][game-1] = gradient\n",
    "    \n",
    "    if game % checkpoint == 0 or game == GAMES:\n",
    "\n",
    "        _mean_steps = metrics[\"steps\"][max(0, game-checkpoint-1):game-1].mean()\n",
    "        _mean_gradient = metrics[\"gradients\"][max(0, game-checkpoint-1):game-1].mean()\n",
    "        \n",
    "        print(f\"Game {game:>6} {int(game/GAMES * 100):>16} % \\n\"\n",
    "              f\"{'-'*30} \\n\"\n",
    "              f\" > Average steps: {int(_mean_steps):>12} \\n\"\n",
    "              f\" > Average gradients: {_mean_gradient:>8.4f} \\n \")\n",
    "        \n",
    "print(f\"Total training time: {time.time()-start:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T11:57:02.847781Z",
     "start_time": "2024-02-08T11:56:11.698871Z"
    }
   },
   "id": "b4abfe2d67240d1f",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visualisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e6dd43949968ed9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def moving_average(data, window_size=50):\n",
    "    \"\"\"Compute moving average with given window size of the data.\"\"\"\n",
    "    half_window = window_size // 2\n",
    "    return [(data[max(0, i-half_window):min(GAMES, i+half_window)]).mean() \n",
    "            for i in range(GAMES)]\n",
    "\n",
    "steps = moving_average(metrics[\"steps\"])\n",
    "gradients = moving_average(metrics[\"gradients\"])\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "ax[0].axhline(y=500, color=\"red\", linestyle=\"dotted\", linewidth=1)\n",
    "ax[0].plot(steps, color=\"black\", linewidth=1)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_title(\"Average steps per game\")\n",
    "\n",
    "ax[1].axhline(y=0, color=\"red\", linestyle=\"dotted\", linewidth=1)\n",
    "ax[1].plot(gradients, color=\"black\", linewidth=1)\n",
    "ax[1].set_xlabel(\"Game nr.\")\n",
    "ax[1].set_title(\"Average gradients\")\n",
    "\n",
    "for i in range(0, GAMES, 1000):\n",
    "    ax[0].axvline(x=i, color='gray', linewidth=0.5)\n",
    "    ax[1].axvline(x=i, color='gray', linewidth=0.5)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T11:57:02.852472Z",
     "start_time": "2024-02-08T11:57:02.850105Z"
    }
   },
   "id": "510380cc5f3c5685",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### In action"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96a3a9859771c84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "state = torch.tensor(environment.reset()[0], dtype=torch.float32)\n",
    "\n",
    "images = []\n",
    "terminated = truncated = False\n",
    "while not (terminated or truncated):\n",
    "    actions = torch.softmax(policy_agent(state), dim=-1)\n",
    "    action = torch.argmax(actions).item()\n",
    "    \n",
    "    state, reward, terminated, truncated, _ = environment.step(action)\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "    images.append(environment.render())\n",
    "_ = imageio.mimsave('./p-cart-pole.gif', images, duration=25)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-08T11:57:02.851179Z"
    }
   },
   "id": "4183f64c47eb5090",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./policy-based-gradient.gif\" width=\"400\" height=\"400\" />"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9f592c07276b7f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "environment.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-08T11:57:02.852187Z"
    }
   },
   "id": "2f63ff4b67fe2c7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (Atari) Breakout environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b91f6c96ad960f00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| | TYPE  | VALUES       | DESCRIPTION |\n",
    "|--------|------------------|--------------|-------------|\n",
    "| Action | ndarray<br/>(1,) | {0, 1, 2, 3} | 0: noop<br/>1: fire<br/>2: right<br/> 3: left |\n",
    "| Observation | ndarray<br/>(210,160) | int          | Grayscale image of screen |\n",
    "| Reward |   | int          | Reward depends on brick colour<br/>Red: 7<br/>Orange: 7<br/>Yellow: 4<br/>Green: 4<br/>Aqua: 1<br/>Blue: 1 |\n",
    "| Termination |   | bool         | Timer: score as many points within the given time |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff07d4d86ec5324e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "# import ale_py.env\n",
    "\n",
    "gym.register(\n",
    "    id=\"ALE/Breakout-v5\",\n",
    "    entry_point=\"ale_py.env.ALEEnv\",\n",
    "    kwargs={\"game\": \"breakout\", \"mode\": \"train\", \"frame_skip\": 4}\n",
    ")\n",
    "\n",
    "environment = gym.make(\"ALE/Breakout-v5\", obs_type=\"grayscale\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T11:57:02.852908Z",
     "start_time": "2024-02-08T11:57:02.852887Z"
    }
   },
   "id": "69abb20079a0b00d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-08T11:57:02.853498Z"
    }
   },
   "id": "55fb475080fb692f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
