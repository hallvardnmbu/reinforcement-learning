{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Policy-based Agent using PyTorch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53c89e80d06ac9e1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-07T10:25:43.665509Z",
     "start_time": "2024-02-07T10:25:42.764084Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from agents import PolicyGradientAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cart-pole environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "970db53c864ab740"
  },
  {
   "cell_type": "markdown",
   "source": [
    "|        | TYPE             | VALUES | DESCRIPTION                                                                     |\n",
    "|--------|------------------|--------|---------------------------------------------------------------------------------|\n",
    "| Action | ndarray<br/>(1,) | {0, 1} | Direction to push cart.<br/>0: left<br/>1: right                                |\n",
    "| Observation | ndarray<br/>(4,) | float  | 1. Cart position<br/>2. Cart velocity<br/>3. Pole angle<br/>4. Angular velocity |\n",
    "| Reward |  | float | Reward is given for every step taken, including the termination step.<br/>Each step provides a reward of +1 |\n",
    "| Termination &<br/>Truncation |  | boolean | Pole Angle exceeds ± 0.2095 rad<br/>Cart Position exceeds ± 2.4 (i.e., edge of the display)<br/>Episode length is greater than 500 |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "811d6d17402feb84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "environment = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T09:41:04.055254Z",
     "start_time": "2024-02-07T09:41:04.036426Z"
    }
   },
   "id": "82fb48c186edca6",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "502492d920829821"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "GAMES = 10000\n",
    "\n",
    "INPUTS = 4\n",
    "OUTPUTS = 2\n",
    "NODES = [15, 30]\n",
    "\n",
    "OPTIMIZER = {\"optim\": torch.optim.RMSprop, \"lr\": 0.00025}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T09:41:04.055457Z",
     "start_time": "2024-02-07T09:41:04.040426Z"
    }
   },
   "id": "2476ac8633268c19",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "policy_agent = PolicyGradientAgent(\n",
    "        network={\"inputs\": INPUTS, \"outputs\": OUTPUTS, \"nodes\": NODES},\n",
    "        optimizer=OPTIMIZER\n",
    ")\n",
    "\n",
    "checkpoint = GAMES // 10\n",
    "metrics = {\"steps\": torch.zeros(GAMES), \"gradients\": torch.zeros(GAMES)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T09:41:04.460847Z",
     "start_time": "2024-02-07T09:41:04.043216Z"
    }
   },
   "id": "4efdbf0dbc4e327f",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game    600               10 % \n",
      "------------------------------ \n",
      " > Average steps:           23 \n",
      " > Average gradients:  -0.0786 \n",
      " \n",
      "Game   1200               20 % \n",
      "------------------------------ \n",
      " > Average steps:           28 \n",
      " > Average gradients:  -0.2712 \n",
      " \n",
      "Game   1800               30 % \n",
      "------------------------------ \n",
      " > Average steps:           41 \n",
      " > Average gradients:  -0.3144 \n",
      " \n",
      "Game   2400               40 % \n",
      "------------------------------ \n",
      " > Average steps:           58 \n",
      " > Average gradients:  -0.4501 \n",
      " \n",
      "Game   3000               50 % \n",
      "------------------------------ \n",
      " > Average steps:           80 \n",
      " > Average gradients:  -0.5833 \n",
      " \n",
      "Game   3600               60 % \n",
      "------------------------------ \n",
      " > Average steps:          131 \n",
      " > Average gradients:  -0.7824 \n",
      " \n",
      "Game   4200               70 % \n",
      "------------------------------ \n",
      " > Average steps:          226 \n",
      " > Average gradients:  -1.7933 \n",
      " \n",
      "Game   4800               80 % \n",
      "------------------------------ \n",
      " > Average steps:          338 \n",
      " > Average gradients:  -2.5072 \n",
      " \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (terminated \u001B[38;5;129;01mor\u001B[39;00m truncated):\n\u001B[1;32m     12\u001B[0m     steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 13\u001B[0m     action, logarithm \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     observation, reward, terminated, truncated, _ \u001B[38;5;241m=\u001B[39m environment\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m     15\u001B[0m     observation \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(observation, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "File \u001B[0;32m~/Desktop/UNIVERSITET/1 Spesialpensum/Kode/reinforcement-learning/agents.py:184\u001B[0m, in \u001B[0;36mPolicyGradientAgent.action\u001B[0;34m(self, state)\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;124;03mStochastic action selection.\u001B[39;00m\n\u001B[1;32m    169\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;124;03m    Logarithm of the selected action probability.\u001B[39;00m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    182\u001B[0m actions \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msoftmax(\u001B[38;5;28mself\u001B[39m(state), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 184\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoice\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mactions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mactions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    186\u001B[0m logarithm \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlog(actions[action])\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m action, logarithm\n",
      "File \u001B[0;32mnumpy/random/mtrand.pyx:982\u001B[0m, in \u001B[0;36mnumpy.random.mtrand.RandomState.choice\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.12/site-packages/numpy/core/fromnumeric.py:2974\u001B[0m, in \u001B[0;36m_prod_dispatcher\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2960\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2961\u001B[0m \u001B[38;5;124;03m    Return the minimum of an array or minimum along an axis.\u001B[39;00m\n\u001B[1;32m   2962\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2968\u001B[0m \u001B[38;5;124;03m    ndarray.min : equivalent method\u001B[39;00m\n\u001B[1;32m   2969\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   2970\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapreduction(a, np\u001B[38;5;241m.\u001B[39mminimum, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m'\u001B[39m, axis, \u001B[38;5;28;01mNone\u001B[39;00m, out,\n\u001B[1;32m   2971\u001B[0m                           keepdims\u001B[38;5;241m=\u001B[39mkeepdims, initial\u001B[38;5;241m=\u001B[39minitial, where\u001B[38;5;241m=\u001B[39mwhere)\n\u001B[0;32m-> 2974\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_prod_dispatcher\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   2975\u001B[0m                      initial\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, where\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   2976\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (a, out)\n\u001B[1;32m   2979\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_prod_dispatcher)\n\u001B[1;32m   2980\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprod\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue,\n\u001B[1;32m   2981\u001B[0m          initial\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue, where\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for game in range(1, GAMES + 1):\n",
    "    \n",
    "    observation = torch.tensor(environment.reset()[0], dtype=torch.float32)  # noqa\n",
    "    terminated = truncated = False\n",
    "    \n",
    "    # LEARNING FROM GAME\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    steps = 0\n",
    "    while not (terminated or truncated):\n",
    "        steps += 1\n",
    "        action, logarithm = policy_agent.action(observation)\n",
    "        observation, reward, terminated, truncated, _ = environment.step(action)\n",
    "        observation = torch.tensor(observation, dtype=torch.float32)\n",
    "        policy_agent.memorize(logarithm, reward)\n",
    "    gradient = policy_agent.learn()\n",
    "    \n",
    "    # METRICS\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    metrics[\"steps\"][game-1] = steps\n",
    "    metrics[\"gradients\"][game-1] = gradient\n",
    "    \n",
    "    if game % checkpoint == 0 or game == GAMES:\n",
    "\n",
    "        _mean_steps = metrics[\"steps\"][max(0, game-checkpoint-1):game-1].mean()\n",
    "        _mean_gradient = metrics[\"gradients\"][max(0, game-checkpoint-1):game-1].mean()\n",
    "        \n",
    "        print(f\"Game {game:>6} {int(game/GAMES * 100):>16} % \\n\"\n",
    "              f\"{'-'*30} \\n\"\n",
    "              f\" > Average steps: {int(_mean_steps):>12} \\n\"\n",
    "              f\" > Average gradients: {_mean_gradient:>8.4f} \\n \")\n",
    "        \n",
    "print(f\"Total training time: {time.time()-start:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T09:42:13.292159Z",
     "start_time": "2024-02-07T09:41:04.516872Z"
    }
   },
   "id": "b4abfe2d67240d1f",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visualisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e6dd43949968ed9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def moving_average(data, window_size=50):\n",
    "    \"\"\"Compute moving average with given window size of the data.\"\"\"\n",
    "    half_window = window_size // 2\n",
    "    return [(data[max(0, i-half_window):min(GAMES, i+half_window)]).mean() \n",
    "            for i in range(GAMES)]\n",
    "\n",
    "steps = moving_average(metrics[\"steps\"])\n",
    "gradients = moving_average(metrics[\"gradients\"])\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "ax[0].axhline(y=500, color=\"red\", linestyle=\"dotted\", linewidth=1)\n",
    "ax[0].plot(steps, color=\"black\", linewidth=1)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_title(\"Average steps per game\")\n",
    "\n",
    "ax[1].axhline(y=0, color=\"red\", linestyle=\"dotted\", linewidth=1)\n",
    "ax[1].plot(gradients, color=\"black\", linewidth=1)\n",
    "ax[1].set_xlabel(\"Game nr.\")\n",
    "ax[1].set_title(\"Average gradients\")\n",
    "\n",
    "for i in range(0, GAMES, 1000):\n",
    "    ax[0].axvline(x=i, color='gray', linewidth=0.5)\n",
    "    ax[1].axvline(x=i, color='gray', linewidth=0.5)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-07T09:42:13.288134Z"
    }
   },
   "id": "510380cc5f3c5685",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### In action"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96a3a9859771c84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "observation = torch.tensor(environment.reset()[0], dtype=torch.float32)\n",
    "\n",
    "images = []\n",
    "terminated = truncated = False\n",
    "while not (terminated or truncated):\n",
    "    actions = torch.softmax(policy_agent(observation), dim=-1)\n",
    "    action = torch.argmax(actions).item()\n",
    "    \n",
    "    observation, reward, terminated, truncated, _ = environment.step(action)\n",
    "    observation = torch.tensor(observation, dtype=torch.float32)\n",
    "\n",
    "    images.append(environment.render())\n",
    "_ = imageio.mimsave('./p-cart-pole.gif', images, duration=25)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-07T09:42:13.289409Z"
    }
   },
   "id": "4183f64c47eb5090",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./policy-based-gradient.gif\" width=\"400\" height=\"400\" />"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9f592c07276b7f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "environment.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-07T09:42:13.290186Z"
    }
   },
   "id": "2f63ff4b67fe2c7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (Atari) Breakout environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b91f6c96ad960f00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| | TYPE  | VALUES       | DESCRIPTION |\n",
    "|--------|------------------|--------------|-------------|\n",
    "| Action | ndarray<br/>(1,) | {0, 1, 2, 3} | 0: noop<br/>1: fire<br/>2: right<br/> 3: left |\n",
    "| Observation | ndarray<br/>(210,160) | int          | Grayscale image of screen |\n",
    "| Reward |   | int          | Reward depends on brick colour<br/>Red: 7<br/>Orange: 7<br/>Yellow: 4<br/>Green: 4<br/>Aqua: 1<br/>Blue: 1 |\n",
    "| Termination |   | bool         | Timer: score as many points within the given time |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff07d4d86ec5324e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ALE'. Environment registration via importing a module failed. Check whether 'ALE' contains env registration and can be imported.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.12/site-packages/gymnasium/envs/registration.py:498\u001B[0m, in \u001B[0;36m_find_spec\u001B[0;34m(env_id)\u001B[0m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 498\u001B[0m     \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.12/importlib/__init__.py:90\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m     89\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1387\u001B[0m, in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1360\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:1324\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'ALE'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 10\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# import ale_py.env\u001B[39;00m\n\u001B[1;32m      4\u001B[0m gym\u001B[38;5;241m.\u001B[39mregister(\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mALE:Breakout-v5\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      6\u001B[0m     entry_point\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124male_py.env.ALEEnv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      7\u001B[0m     kwargs\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgame\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbreakout\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mframe_skip\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m4\u001B[39m}\n\u001B[1;32m      8\u001B[0m )\n\u001B[0;32m---> 10\u001B[0m environment \u001B[38;5;241m=\u001B[39m \u001B[43mgym\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mALE:Breakout-v5\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobs_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrayscale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.12/site-packages/gymnasium/envs/registration.py:741\u001B[0m, in \u001B[0;36mmake\u001B[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001B[0m\n\u001B[1;32m    738\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mid\u001B[39m, \u001B[38;5;28mstr\u001B[39m)\n\u001B[1;32m    740\u001B[0m     \u001B[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001B[39;00m\n\u001B[0;32m--> 741\u001B[0m     env_spec \u001B[38;5;241m=\u001B[39m \u001B[43m_find_spec\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    743\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(env_spec, EnvSpec)\n\u001B[1;32m    745\u001B[0m \u001B[38;5;66;03m# Update the env spec kwargs with the `make` kwargs\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.12/site-packages/gymnasium/envs/registration.py:500\u001B[0m, in \u001B[0;36m_find_spec\u001B[0;34m(env_id)\u001B[0m\n\u001B[1;32m    498\u001B[0m         importlib\u001B[38;5;241m.\u001B[39mimport_module(module)\n\u001B[1;32m    499\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 500\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m(\n\u001B[1;32m    501\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Environment registration via importing a module failed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    502\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCheck whether \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m contains env registration and can be imported.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    503\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    505\u001B[0m \u001B[38;5;66;03m# load the env spec from the registry\u001B[39;00m\n\u001B[1;32m    506\u001B[0m env_spec \u001B[38;5;241m=\u001B[39m registry\u001B[38;5;241m.\u001B[39mget(env_name)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'ALE'. Environment registration via importing a module failed. Check whether 'ALE' contains env registration and can be imported."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "# import ale_py.env\n",
    "\n",
    "gym.register(\n",
    "    id=\"ALE/Breakout-v5\",\n",
    "    entry_point=\"ale_py.env.ALEEnv\",\n",
    "    kwargs={\"game\": \"breakout\", \"mode\": \"train\", \"frame_skip\": 4}\n",
    ")\n",
    "\n",
    "environment = gym.make(\"ALE/Breakout-v5\", obs_type=\"grayscale\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T10:29:30.773880Z",
     "start_time": "2024-02-07T10:29:30.721577Z"
    }
   },
   "id": "69abb20079a0b00d",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "55fb475080fb692f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
