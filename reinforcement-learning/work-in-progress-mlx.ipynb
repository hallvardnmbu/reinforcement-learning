{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Policy-based agent using Apple MLX"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53c89e80d06ac9e1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-10T18:26:56.009790Z",
     "start_time": "2024-02-10T18:26:55.452846Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import imageio\n",
    "import numpy as np\n",
    "import mlx.nn as nn\n",
    "import mlx.core as mx\n",
    "import gymnasium as gym\n",
    "import mlx.optimizers as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mlx.policy import PolicyGradient"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cart-pole environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "970db53c864ab740"
  },
  {
   "cell_type": "markdown",
   "source": [
    "|        | TYPE             | VALUES | DESCRIPTION                                                                     |\n",
    "|--------|------------------|--------|---------------------------------------------------------------------------------|\n",
    "| Action | ndarray<br/>(1,) | {0, 1} | Direction to push cart.<br/>0: left<br/>1: right                                |\n",
    "| Observation | ndarray<br/>(4,) | float  | 1. Cart position<br/>2. Cart velocity<br/>3. Pole angle<br/>4. Angular velocity |\n",
    "| Reward |  | float | Reward is given for every step taken, including the termination step.<br/>Each step provides a reward of +1 |\n",
    "| Termination &<br/>Truncation |  | boolean | Pole Angle exceeds ± 0.2095 rad<br/>Cart Position exceeds ± 2.4 (i.e., edge of the display)<br/>Episode length is greater than 500 |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "811d6d17402feb84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "environment = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T18:26:56.013601Z",
     "start_time": "2024-02-10T18:26:56.008493Z"
    }
   },
   "id": "82fb48c186edca6",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "502492d920829821"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "GAMES = 10000\n",
    "\n",
    "NETWORK = {\"inputs\": 4, \"outputs\": 2, \"nodes\": [15, 30]}\n",
    "OPTIMIZER = {\"optim\": optim.RMSprop, \"lr\": 0.00025}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T18:26:56.016773Z",
     "start_time": "2024-02-10T18:26:56.014145Z"
    }
   },
   "id": "2476ac8633268c19",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "policy_agent = PolicyGradient(\n",
    "    network=NETWORK, optimizer=OPTIMIZER\n",
    ")\n",
    "\n",
    "checkpoint = GAMES // 10\n",
    "metrics = {metric: mx.array(np.zeros(GAMES)) for metric in [\"steps\", \"gradients\"]}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T18:26:56.020024Z",
     "start_time": "2024-02-10T18:26:56.017303Z"
    }
   },
   "id": "4efdbf0dbc4e327f",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros_like(): incompatible function arguments. The following argument types are supported:\n    1. (a: mlx.core.array, /, *, stream: Union[None, mlx.core.Stream, mlx.core.Device] = None) -> mlx.core.array\n\nInvoked with: <function PolicyGradient.learn at 0x11bc3c860>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 18\u001B[0m\n\u001B[1;32m     16\u001B[0m     policy_agent\u001B[38;5;241m.\u001B[39mmemorize(logarithm, reward)\n\u001B[1;32m     17\u001B[0m gradient \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mvalue_and_grad(policy_agent, policy_agent\u001B[38;5;241m.\u001B[39mlearn)\n\u001B[0;32m---> 18\u001B[0m \u001B[43mpolicy_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpolicy_agent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# METRICS\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# --------------------------------------------------\u001B[39;00m\n\u001B[1;32m     23\u001B[0m metrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msteps\u001B[39m\u001B[38;5;124m\"\u001B[39m][game\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m steps\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.12/site-packages/mlx/optimizers.py:28\u001B[0m, in \u001B[0;36mOptimizer.update\u001B[0;34m(self, model, gradients)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate\u001B[39m(\u001B[38;5;28mself\u001B[39m, model: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmlx.nn.Module\u001B[39m\u001B[38;5;124m\"\u001B[39m, gradients: \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m     20\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Apply the gradients to the parameters of the model and update the\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;124;03m    model with the new parameters.\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;124;03m                          via :func:`mlx.nn.value_and_grad`.\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m     model\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgradients\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.12/site-packages/mlx/optimizers.py:78\u001B[0m, in \u001B[0;36mOptimizer.apply_gradients\u001B[0;34m(self, gradients, parameters)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Apply the gradients to the parameters and return the updated parameters.\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \n\u001B[1;32m     67\u001B[0m \u001B[38;5;124;03mCan be used to update a model via\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;124;03m      tree will be of the same structure as the gradients.\u001B[39;00m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_initialized:\n\u001B[0;32m---> 78\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgradients\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tree_map(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_single, gradients, parameters, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate)\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.12/site-packages/mlx/optimizers.py:52\u001B[0m, in \u001B[0;36mOptimizer.init\u001B[0;34m(self, parameters)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Initialize the optimizer's state\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \n\u001B[1;32m     33\u001B[0m \u001B[38;5;124;03mThis function can be used to initialize optimizers which have state\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m           [0, 0]], dtype=float32)}, 'bias': {'v': array([0, 0], dtype=float32)}}\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state\u001B[38;5;241m.\u001B[39mupdate(tree_map(\u001B[38;5;28;01mlambda\u001B[39;00m x: {}, parameters))\n\u001B[0;32m---> 52\u001B[0m \u001B[43mtree_map\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit_single\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_initialized \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.12/site-packages/mlx/utils.py:54\u001B[0m, in \u001B[0;36mtree_map\u001B[0;34m(fn, tree, is_leaf, *rest)\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m     50\u001B[0m         k: tree_map(fn, child, \u001B[38;5;241m*\u001B[39m(r[k] \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m rest), is_leaf\u001B[38;5;241m=\u001B[39mis_leaf)\n\u001B[1;32m     51\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, child \u001B[38;5;129;01min\u001B[39;00m tree\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m     52\u001B[0m     }\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtree\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrest\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.12/site-packages/mlx/optimizers.py:212\u001B[0m, in \u001B[0;36mRMSprop.init_single\u001B[0;34m(self, parameter, state)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minit_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, parameter: mx\u001B[38;5;241m.\u001B[39marray, state: \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    211\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Initialize optimizer state\"\"\"\u001B[39;00m\n\u001B[0;32m--> 212\u001B[0m     state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mv\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mmx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparameter\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: zeros_like(): incompatible function arguments. The following argument types are supported:\n    1. (a: mlx.core.array, /, *, stream: Union[None, mlx.core.Stream, mlx.core.Device] = None) -> mlx.core.array\n\nInvoked with: <function PolicyGradient.learn at 0x11bc3c860>"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for game in range(1, GAMES + 1):\n",
    "    \n",
    "    state = mx.array(environment.reset()[0])  # noqa\n",
    "    terminated = truncated = False\n",
    "    \n",
    "    # LEARNING FROM GAME\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    steps = 0\n",
    "    while not (terminated or truncated):\n",
    "        steps += 1\n",
    "        action, logarithm = policy_agent.action(state)\n",
    "        state, reward, terminated, truncated, _ = environment.step(action)\n",
    "        state = mx.array(state)\n",
    "        policy_agent.memorize(logarithm, reward)\n",
    "    gradient = policy_agent.learn()\n",
    "    \n",
    "    # METRICS\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    metrics[\"steps\"][game-1] = steps\n",
    "    metrics[\"gradients\"][game-1] = gradient\n",
    "\n",
    "    if game % checkpoint == 0 or game == GAMES:\n",
    "        _mean_steps = metrics[\"steps\"][max(0, game-checkpoint-1):game-1].mean()\n",
    "        _mean_gradient = metrics[\"gradients\"][max(0, game-checkpoint-1):game-1].mean()\n",
    "\n",
    "        print(f\"Game {game:>6} {int(game/GAMES * 100):>16} % \\n\"\n",
    "              f\"{'-'*30} \\n\"\n",
    "              f\" > Average steps: {int(_mean_steps):>12} \\n\"\n",
    "              f\" > Average gradients: {_mean_gradient:>8.4f} \\n \")\n",
    "\n",
    "print(f\"Total training time: {time.time()-start:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T18:26:56.162928Z",
     "start_time": "2024-02-10T18:26:56.020902Z"
    }
   },
   "id": "b4abfe2d67240d1f",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visualisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e6dd43949968ed9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def moving_average(data, window_size=50):\n",
    "    \"\"\"Compute moving average with given window size of the data.\"\"\"\n",
    "    half_window = window_size // 2\n",
    "    return [(data[max(0, i-half_window):min(GAMES, i+half_window)]).mean() \n",
    "            for i in range(GAMES)]\n",
    "\n",
    "steps = moving_average(metrics[\"steps\"])\n",
    "gradients = moving_average(metrics[\"gradients\"])\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "fig.suptitle(\"Policy-based gradient agent\")\n",
    "\n",
    "ax[0].axhline(y=500, color=\"red\", linestyle=\"dotted\", linewidth=1)\n",
    "ax[0].plot(steps, color=\"black\", linewidth=1)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_title(\"Average steps per game\")\n",
    "\n",
    "ax[1].axhline(y=0, color=\"red\", linestyle=\"dotted\", linewidth=1)\n",
    "ax[1].plot(gradients, color=\"black\", linewidth=1)\n",
    "ax[1].set_xlabel(\"Game nr.\")\n",
    "ax[1].set_title(\"Average gradients\")\n",
    "\n",
    "for i in range(0, GAMES, GAMES // 10):\n",
    "    ax[0].axvline(x=i, color='gray', linewidth=0.5)\n",
    "    ax[1].axvline(x=i, color='gray', linewidth=0.5)\n",
    "\n",
    "plt.savefig(\"./static/images/torch-pbg.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-10T18:26:56.161436Z"
    }
   },
   "id": "510380cc5f3c5685",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### In action"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96a3a9859771c84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "state = mx.array(environment.reset()[0])\n",
    "\n",
    "images = []\n",
    "terminated = truncated = False\n",
    "while not (terminated or truncated):\n",
    "    actions = nn.softmax(policy_agent(state))\n",
    "    action = actions.argmax().item()\n",
    "    \n",
    "    state, reward, terminated, truncated, _ = environment.step(action)\n",
    "    state = mx.array(state)\n",
    "\n",
    "    images.append(environment.render())\n",
    "_ = imageio.mimsave('./static/images/torch-pbg.gif', images, duration=25)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-10T18:26:56.162874Z"
    }
   },
   "id": "4183f64c47eb5090",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./static/images/torch-pbg.gif\" width=\"1000\" height=\"1000\" />"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9f592c07276b7f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "environment.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-10T18:26:56.163684Z"
    }
   },
   "id": "2f63ff4b67fe2c7",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
