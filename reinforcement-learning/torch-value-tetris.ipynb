{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Value-based agent using PyTorch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53c89e80d06ac9e1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-16T15:15:08.010234Z",
     "start_time": "2024-02-16T15:15:06.904223Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from agents.torch_value_vision import VisionDeepQ"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tetris environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "970db53c864ab740"
  },
  {
   "cell_type": "markdown",
   "source": [
    "|        | TYPE                   | VALUES          | DESCRIPTION                                                                                                |\n",
    "|--------|------------------------|-----------------|------------------------------------------------------------------------------------------------------------|\n",
    "| Action Space | ndarray<br/>(1,) | {0, 1, 2, 3, 4} | Action to manipulate the current tile.<br/>0: No action<br/>1: Rotate<br/>2: Right<br/>3: Left<br/>4: Down |\n",
    "| Observation Space | ndarray<br/>(210,160,) | <0, 255> | The game screen.                                                                                           |\n",
    "| Reward |  | float | Reward given when a row is filled.<br/>Single: 50<br/>Double: 150<br/>Triple: 400<br/>Quadruple: 900       |\n",
    "| Termination |  | boolean | The game ends when the pieces stack up to the top of the playing field.                                    |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "811d6d17402feb84"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "environment = gym.make('ALE/Tetris-v5', render_mode=\"rgb_array\", obs_type=\"grayscale\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T15:15:08.064612Z",
     "start_time": "2024-02-16T15:15:08.010978Z"
    }
   },
   "id": "82fb48c186edca6",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "NOTE:\n",
    "\n",
    "The mini-batch value represents how many _full_ games to train on from the agents memor. \n",
    "Therefore, an additional parameter `TRAIN_EVERY` is introduced to control how often the agent \n",
    "should learn from the memorized games. Although, even with this parameter, the training time \n",
    "increases significantly as the agent improves and has to learn from longer and longer games."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "502492d920829821"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "GAMES = 50\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "EXPLORATION_RATE = 1.0\n",
    "EXPLORATION_DECAY = 0.995\n",
    "EXPLORATION_MIN = 0.01\n",
    "\n",
    "MINIBATCH = 64\n",
    "TRAIN_EVERY = 10\n",
    "\n",
    "MEMORY = 1500\n",
    "RESET_Q_EVERY = 250\n",
    "\n",
    "NETWORK = {\"input_channels\": 1, \"outputs\": 5, \n",
    "           \"channels\": [4, 8],\n",
    "           \"kernels\": [(3, 3), (3, 3), (5, 5)]\n",
    "           }\n",
    "OPTIMIZER = {\"optim\": torch.optim.RMSprop, \"lr\": 0.00025}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T15:15:08.064877Z",
     "start_time": "2024-02-16T15:15:08.062947Z"
    }
   },
   "id": "2476ac8633268c19",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "value_agent = VisionDeepQ(\n",
    "    network=NETWORK, optimizer=OPTIMIZER,\n",
    "    gamma=GAMMA, batch_size=MINIBATCH, memory=MEMORY,\n",
    "    exploration_rate=EXPLORATION_RATE, exploration_decay=EXPLORATION_DECAY, exploration_min=EXPLORATION_MIN\n",
    ")\n",
    "\n",
    "_value_agent = copy.deepcopy(value_agent)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T15:15:08.466277Z",
     "start_time": "2024-02-16T15:15:08.066311Z"
    }
   },
   "id": "4efdbf0dbc4e327f",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "checkpoint = GAMES // 10\n",
    "metrics = {\n",
    "    \"steps\": torch.zeros(GAMES),\n",
    "    \"losses\": torch.zeros(GAMES // TRAIN_EVERY),\n",
    "    \"exploration\": torch.zeros(GAMES)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T15:15:08.470653Z",
     "start_time": "2024-02-16T15:15:08.466914Z"
    }
   },
   "id": "3e9591009b6a7d65",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game      5               10 % \n",
      "------------------------------ \n",
      " > Average steps:          522 \n",
      " > Average loss:           nan \n",
      " \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [4, 1, 3, 3], expected input[1, 5313, 210, 160] to have 1 channels, but got 5313 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 22\u001B[0m\n\u001B[1;32m     19\u001B[0m value_agent\u001B[38;5;241m.\u001B[39mmemorize(steps)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m game \u001B[38;5;241m%\u001B[39m TRAIN_EVERY \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 22\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mvalue_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_value_agent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m     metrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlosses\u001B[39m\u001B[38;5;124m\"\u001B[39m][game \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m TRAIN_EVERY \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m loss\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m game \u001B[38;5;241m%\u001B[39m RESET_Q_EVERY \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Desktop/UNIVERSITET/1 Spesialpensum/Kode/reinforcement-learning/agents/torch_value_vision.py:223\u001B[0m, in \u001B[0;36mVisionDeepQ.learn\u001B[0;34m(self, network)\u001B[0m\n\u001B[1;32m    203\u001B[0m rewards \u001B[38;5;241m=\u001B[39m ((rewards \u001B[38;5;241m-\u001B[39m rewards\u001B[38;5;241m.\u001B[39mmean()) \u001B[38;5;241m/\u001B[39m (rewards\u001B[38;5;241m.\u001B[39mstd() \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1e-9\u001B[39m))\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    205\u001B[0m \u001B[38;5;66;03m# Q-LEARNING\u001B[39;00m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------------------------\u001B[39;00m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;66;03m# The Q-learning implementation is based on the letter by Google DeepMind (\"Human-level\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    220\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;66;03m# where Q' is a copy of the agent, which is updated every C steps.\u001B[39;00m\n\u001B[0;32m--> 223\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mstates\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m    225\u001B[0m actual \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m(states)\u001B[38;5;241m.\u001B[39mgather(\u001B[38;5;241m1\u001B[39m, actions\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m    227\u001B[0m optimal \u001B[38;5;241m=\u001B[39m (rewards \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m    228\u001B[0m            \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgamma \u001B[38;5;241m*\u001B[39m network(new_states)\u001B[38;5;241m.\u001B[39mmax(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/UNIVERSITET/1 Spesialpensum/Kode/reinforcement-learning/agents/torch_value_vision.py:122\u001B[0m, in \u001B[0;36mVisionDeepQ.forward\u001B[0;34m(self, state)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, state):\n\u001B[1;32m    110\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;124;03m    Forward pass with nonmodified output.\u001B[39;00m\n\u001B[1;32m    112\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;124;03m    output : torch.Tensor\u001B[39;00m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     _output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer_0\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m    125\u001B[0m         _output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayer_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)(_output))\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/spesialpensum/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    454\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    455\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Given groups=1, weight of size [4, 1, 3, 3], expected input[1, 5313, 210, 160] to have 1 channels, but got 5313 channels instead"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for game in range(1, GAMES + 1):\n",
    "    \n",
    "    state = torch.tensor(environment.reset()[0], dtype=torch.float32).unsqueeze(0)  # noqa\n",
    "    terminated = truncated = False\n",
    "    \n",
    "    # LEARNING FROM GAME\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    steps = 0\n",
    "    while not (terminated or truncated):\n",
    "        steps += 1\n",
    "        action = value_agent.action(state)\n",
    "        new_state, reward, terminated, truncated, _ = environment.step(action.item())\n",
    "        new_state = torch.tensor(new_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        value_agent.remember(state, action, new_state, torch.tensor([reward]))\n",
    "        state = new_state\n",
    "    value_agent.memorize(steps)\n",
    "    \n",
    "    if game % TRAIN_EVERY == 0:\n",
    "        loss = value_agent.learn(network=_value_agent)\n",
    "        metrics[\"losses\"][game // TRAIN_EVERY - 1] = loss\n",
    "    \n",
    "    if game % RESET_Q_EVERY == 0:\n",
    "        _value_agent.load_state_dict(value_agent.state_dict())\n",
    "\n",
    "    # METRICS\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    metrics[\"steps\"][game-1] = steps\n",
    "    metrics[\"exploration\"][game-1] = value_agent.explore[\"rate\"]\n",
    "    \n",
    "    if game % checkpoint == 0 or game == GAMES:\n",
    "        _mean_steps = metrics[\"steps\"][max(0, game-checkpoint-1):game-1].mean()\n",
    "        _mean_loss = metrics[\"losses\"][max(0, (game-checkpoint-1) // TRAIN_EVERY):game // TRAIN_EVERY].mean()\n",
    "        \n",
    "        print(f\"Game {game:>6} {int(game/GAMES * 100):>16} % \\n\"\n",
    "              f\"{'-'*30} \\n\"\n",
    "              f\" > Average steps: {int(_mean_steps):>12} \\n\"\n",
    "              f\" > Average loss: {_mean_loss:>13.4f} \\n \")\n",
    "        \n",
    "print(f\"Total training time: {time.time()-start:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T15:15:12.379991Z",
     "start_time": "2024-02-16T15:15:08.478867Z"
    }
   },
   "id": "b4abfe2d67240d1f",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visualisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e6dd43949968ed9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def moving_average(data, window_size=50):\n",
    "    \"\"\"Compute moving average with given window size of the data.\"\"\"\n",
    "    half_window = window_size // 2\n",
    "    return [(data[max(0, i-half_window):min(len(data), i+half_window)]).mean() \n",
    "            for i in range(len(data))]\n",
    "\n",
    "steps = moving_average(metrics[\"steps\"])\n",
    "losses = moving_average(metrics[\"losses\"])\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "fig.suptitle(\"Deep Q-Learning vision agent\")\n",
    "\n",
    "ax[0].axhline(y=500, color=\"red\", linestyle=\"dotted\", linewidth=1)\n",
    "ax[0].plot(steps, color=\"black\", linewidth=1)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_title(\"Average steps per game\")\n",
    "\n",
    "ax[1].plot(torch.linspace(0, GAMES, len(losses)), losses, color=\"black\", linewidth=1)\n",
    "ax[1].set_xlabel(\"Game nr.\")\n",
    "ax[1].set_title(\"Average loss\")\n",
    "\n",
    "ax_2 = ax[1].twinx()\n",
    "ax_2.plot(metrics[\"exploration\"], color=\"gray\", linewidth=0.5)\n",
    "ax_2.set_ylabel(\"Exploration rate\")\n",
    "ax_2.yaxis.label.set_color('gray')\n",
    "ax_2.tick_params(axis='y', colors='gray')\n",
    "\n",
    "for i in range(0, GAMES, GAMES // 10):\n",
    "    ax[0].axvline(x=i, color='gray', linewidth=0.5)\n",
    "    ax[1].axvline(x=i, color='gray', linewidth=0.5)\n",
    "\n",
    "plt.savefig(\"./static/images/torch-dqn-tetris.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-16T15:15:12.379479Z"
    }
   },
   "id": "510380cc5f3c5685",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### In action"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96a3a9859771c84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "state = torch.tensor(environment.reset()[0], dtype=torch.float32)\n",
    "\n",
    "images = []\n",
    "terminated = truncated = False\n",
    "while not (terminated or truncated):\n",
    "    action = torch.argmax(value_agent(state)).item()\n",
    "    \n",
    "    state, reward, terminated, truncated, _ = environment.step(action)\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "    images.append(environment.render())\n",
    "_ = imageio.mimsave('./static/images/torch-dqn-tetris.gif', images, duration=25)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T15:15:12.382048Z",
     "start_time": "2024-02-16T15:15:12.381232Z"
    }
   },
   "id": "4183f64c47eb5090",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./static/images/torch-dqn-tetris.gif\" width=\"1000\" height=\"1000\" />"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9f592c07276b7f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "environment.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T15:15:12.382289Z",
     "start_time": "2024-02-16T15:15:12.382109Z"
    }
   },
   "id": "2f63ff4b67fe2c7",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
