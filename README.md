Modern applied deep learning with reinforcement and Transformer model methodology

---

Special syllabus at NMBU (Norwegian University of Life Sciences)
Spring 2024

* Hallvard H. Lavik
* Leo Q. T. Bækholt

---

Syllabus:

Reinforcement Learning:
- "Human-level control through deep reinforcement learning" (doi:10.1038/nature14236)
- "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm" (arXiv:1712.01815v1)

Transformer:
- "Geometry of deep learning" (ISBN 978-981-16-6046-7)
  - Chapter 9.3 ("Attention")
  - Chapter 9.4.5 ("Transformer")
  - Chapter 9.4.7 ("Generative Pre-trained Transformer (GPT)")
- "Attention Is All You Need" (arXiv:1706.03762v7)
- "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (arXiv:1810.04805v2)
- "An image is worth 16x16 words: Transformers for image recognition at scale" (arXiv:2010.11929v2)

---

Learning goals:

- Understand and know how to build, use and deploy reinforcement learning algorithms
  * Experiment with reinforcement agent(s) (for instance playing chess)
- Understand and know how to build, use and deploy Transformer architectures
  * Experiment with architectures and applications (for instance, a language translator)

---

Learning outcomes:

- Be competent in modern deep learning situations
  * Understand (and to some extent be able to reproduce) cutting-edge “artificial intelligence” models
